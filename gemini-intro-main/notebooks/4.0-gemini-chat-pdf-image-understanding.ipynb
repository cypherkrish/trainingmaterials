{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini App Chatting with PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-01 12:23:59--  https://arxiv.org/pdf/2308.03688\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23176585 (22M) [application/pdf]\n",
      "Saving to: ‘2308.03688’\n",
      "\n",
      "2308.03688          100%[===================>]  22.10M   105MB/s    in 0.2s    \n",
      "\n",
      "2025-05-01 12:23:59 (105 MB/s) - ‘2308.03688’ saved [23176585/23176585]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2308.03688 && mv 2308.03688 ./paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the document:\n",
      "\n",
      "The paper introduces AgentBench, a new multi-dimensional benchmark designed to evaluate Large Language Models (LLMs) as agents in complex interactive environments. AgentBench consists of 8 distinct environments that assess an LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. The evaluation of 27 LLMs (API-based and open-sourced) on AgentBench reveals a significant disparity in performance between commercial LLMs and OSS competitors, with commercial LLMs demonstrating a strong ability to act as agents in complex environments. The paper identifies the typical reasons for failures, highlighting poor long-term reasoning, decision-making, and instruction following abilities as the main obstacles for developing usable LLM agents. The authors suggest that training on code and high-quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released to the public.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "doc_url = \"https://arxiv.org/pdf/2308.03688\"\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(doc_url).content\n",
    "\n",
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"table_num\": 1,\n",
      "    \"title\": \"AGENTBENCH evaluates 27 API-based or OSS LLMs on LLM-as-Agent challenges\",\n",
      "    \"columns\": [\n",
      "      \"Model\",\n",
      "      \"#Size\",\n",
      "      \"Form\",\n",
      "      \"Ver.\",\n",
      "      \"Creator\",\n",
      "      \"Model\",\n",
      "      \"#Size\",\n",
      "      \"Form\",\n",
      "      \"Ver.\",\n",
      "      \"Creator\"\n",
      "    ],\n",
      "    \"rows\": [\n",
      "      [\n",
      "        \"gpt-4 (OpenAI, 2023)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"0613\",\n",
      "        \"OpenAI\",\n",
      "        \"text-davinci-002 (Ouyang et al., 2022)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"\",\n",
      "        \"OpenAI\"\n",
      "      ],\n",
      "      [\n",
      "        \"gpt-3.5-turbo (OpenAI, 2022)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"0613\",\n",
      "        \"OpenAI\",\n",
      "        \"llama2-70b (Touvron et al., 2023)\",\n",
      "        \"70B\",\n",
      "        \"open chat\",\n",
      "        \"\",\n",
      "        \"Meta\"\n",
      "      ],\n",
      "      [\n",
      "        \"text-davinci-003 (Ouyang et al., 2022)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"\",\n",
      "        \"OpenAI\",\n",
      "        \"llama2-13b (Touvron et al., 2023)\",\n",
      "        \"13B\",\n",
      "        \"open chat\",\n",
      "        \"\",\n",
      "        \"Meta\"\n",
      "      ],\n",
      "      [\n",
      "        \"claude-2 (Anthropic, 2023b)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"\",\n",
      "        \"Anthropic\",\n",
      "        \"llama2-7b (Touvron et al., 2023)\",\n",
      "        \"7B\",\n",
      "        \"open chat\",\n",
      "        \"\",\n",
      "        \"Meta\"\n",
      "      ],\n",
      "      [\n",
      "        \"claude (Anthropic, 2023a)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"v1.3\",\n",
      "        \"Anthropic\",\n",
      "        \"guanaco-65b (Dettmers et al., 2023)\",\n",
      "        \"65B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\"\n",
      "      ],\n",
      "      [\n",
      "        \"claude-instant (Anthropic, 2023a)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"v1.1\",\n",
      "        \"Anthropic\",\n",
      "        \"guanaco-33b (Dettmers et al., 2023)\",\n",
      "        \"33B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\"\n",
      "      ],\n",
      "      [\n",
      "        \"chat-bison-001 (Anil et al., 2023)\",\n",
      "        \"N/A\",\n",
      "        \"api\",\n",
      "        \"\",\n",
      "        \"Google\",\n",
      "        \"vicuna-33b (Chiang et al., 2023)\",\n",
      "        \"33B\",\n",
      "        \"open\",\n",
      "        \"v1.3\",\n",
      "        \"LMSYS\"\n",
      "      ],\n",
      "      [\n",
      "        \"chatglm-6b (Zeng et al., 2022; Du et al., 2022)\",\n",
      "        \"6B\",\n",
      "        \"open\",\n",
      "        \"v1.1\",\n",
      "        \"Tsinghua& Zhipu\",\n",
      "        \"vicuna-13b (Chiang et al., 2023)\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"v1.5\",\n",
      "        \"LMSYS\"\n",
      "      ],\n",
      "      [\n",
      "        \"codegeex2-6b (Zheng et al., 2023)\",\n",
      "        \"6B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"openchat-13b (Wang et al., 2023a)\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"v3.2\",\n",
      "        \"Tsinghua\"\n",
      "      ],\n",
      "      [\n",
      "        \"codellama-34b (Rozière et al., 2023)\",\n",
      "        \"34B\",\n",
      "        \"open\",\n",
      "        \"instruct\",\n",
      "        \"Meta\",\n",
      "        \"wizardlm-30b (Xu et al., 2023)\",\n",
      "        \"30B\",\n",
      "        \"open\",\n",
      "        \"v1.0\",\n",
      "        \"Microsoft\"\n",
      "      ],\n",
      "      [\n",
      "        \"codellama-13b (Rozière et al., 2023)\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"instruct\",\n",
      "        \"Meta\",\n",
      "        \"koala-13b (Geng et al., 2023)\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"v1.0\",\n",
      "        \"UCB\"\n",
      "      ],\n",
      "      [\n",
      "        \"codellama-7b (Rozière et al., 2023)\",\n",
      "        \"7B\",\n",
      "        \"open\",\n",
      "        \"instruct\",\n",
      "        \"Meta\",\n",
      "        \"oasst-12b (LAION, 2023)\",\n",
      "        \"12B\",\n",
      "        \"open\",\n",
      "        \"sft-4\",\n",
      "        \"LAION\"\n",
      "      ],\n",
      "      [\n",
      "        \"dolly-12b (Conover et al., 2023)\",\n",
      "        \"12B\",\n",
      "        \"open\",\n",
      "        \"v2\",\n",
      "        \"Databricks\",\n",
      "        \"wizardlm-13b (Xu et al., 2023)\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"llama-2-13b\",\n",
      "        \"13B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"\",\n",
      "        \"Ilama-2-7b\",\n",
      "        \"7B\",\n",
      "        \"open\",\n",
      "        \"\",\n",
      "        \"\"\n",
      "      ]\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"table_num\": 2,\n",
      "    \"title\": \"Statistics and metrics of 8 environments in AGENTBENCH evaluation\",\n",
      "    \"columns\": [\n",
      "      \"\",\n",
      "      \"OS\",\n",
      "      \"DB\",\n",
      "      \"KG\",\n",
      "      \"DCG\",\n",
      "      \"LTP\",\n",
      "      \"HH\",\n",
      "      \"WS\",\n",
      "      \"WB\"\n",
      "    ],\n",
      "    \"rows\": [\n",
      "      [\n",
      "        \"#Avg. Turn\",\n",
      "        \"8\",\n",
      "        \"5\",\n",
      "        \"15\",\n",
      "        \"30\",\n",
      "        \"22\",\n",
      "        \"35\",\n",
      "        \"5\",\n",
      "        \"10\"\n",
      "      ],\n",
      "      [\n",
      "        \"Metric\",\n",
      "        \"SR\",\n",
      "        \"SR\",\n",
      "        \"F1\",\n",
      "        \"Reward\",\n",
      "        \"Game Progress\",\n",
      "        \"SR\",\n",
      "        \"Reward\",\n",
      "        \"Step SR\"\n",
      "      ],\n",
      "      [\n",
      "        \"#Dev\",\n",
      "        \"26/240\",\n",
      "        \"60/300\",\n",
      "        \"20/300\",\n",
      "        \"12/360\",\n",
      "        \"20/500\",\n",
      "        \"20/700\",\n",
      "        \"80/400\",\n",
      "        \"31/400\"\n",
      "      ],\n",
      "      [\n",
      "        \"#Test\",\n",
      "        \"144/1200\",\n",
      "        \"300/1500\",\n",
      "        \"150/2250\",\n",
      "        \"20/600\",\n",
      "        \"50/1250\",\n",
      "        \"50/1750\",\n",
      "        \"200/1000\",\n",
      "        \"177/1800\"\n",
      "      ],\n",
      "      [\n",
      "        \"Weight1\",\n",
      "        \"10.8\",\n",
      "        \"13.0\",\n",
      "        \"13.9\",\n",
      "        \"12.0\",\n",
      "        \"3.5\",\n",
      "        \"13.0\",\n",
      "        \"30.7\",\n",
      "        \"11.6\"\n",
      "      ]\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"table_num\": 3,\n",
      "    \"title\": \"Test set (standard) results of AGENTBENCH\",\n",
      "    \"columns\": [\n",
      "      \"LLM Type\",\n",
      "      \"Models\",\n",
      "      \"VER\",\n",
      "      \"OA\",\n",
      "      \"Code-grounded\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"Game-grounded\",\n",
      "      \"\",\n",
      "      \"Web-grounded\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"OS\",\n",
      "      \"DB\",\n",
      "      \"KG\",\n",
      "      \"DCG\",\n",
      "      \"LTP\",\n",
      "      \"HH\",\n",
      "      \"WS\",\n",
      "      \"WB\"\n",
      "    ],\n",
      "    \"rows\": [\n",
      "      [\n",
      "        \"\",\n",
      "        \"gpt-4\",\n",
      "        \"0613\",\n",
      "        \"4.01\",\n",
      "        \"42.4\",\n",
      "        \"32.0\",\n",
      "        \"58.8\",\n",
      "        \"74.5\",\n",
      "        \"16.6\",\n",
      "        \"78.0\",\n",
      "        \"61.1\",\n",
      "        \"29.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"claude-2\",\n",
      "        \"\",\n",
      "        \"2.49\",\n",
      "        \"18.1\",\n",
      "        \"27.3\",\n",
      "        \"41.3\",\n",
      "        \"55.5\",\n",
      "        \"8.4\",\n",
      "        \"54.0\",\n",
      "        \"61.4\",\n",
      "        \"0.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"claude\",\n",
      "        \"v1.3\",\n",
      "        \"2.44\",\n",
      "        \"9.7\",\n",
      "        \"22.0\",\n",
      "        \"38.9\",\n",
      "        \"40.9\",\n",
      "        \"8.2\",\n",
      "        \"58.0\",\n",
      "        \"55.7\",\n",
      "        \"25.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"API\",\n",
      "        \"gpt-3.5-turbo\",\n",
      "        \"0613\",\n",
      "        \"2.32\",\n",
      "        \"32.6\",\n",
      "        \"36.7\",\n",
      "        \"25.9\",\n",
      "        \"33.7\",\n",
      "        \"10.5\",\n",
      "        \"16.0\",\n",
      "        \"64.1\",\n",
      "        \"20.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"text-davinci-003\",\n",
      "        \"\",\n",
      "        \"1.71\",\n",
      "        \"20.1\",\n",
      "        \"16.3\",\n",
      "        \"34.9\",\n",
      "        \"3.0\",\n",
      "        \"7.1\",\n",
      "        \"20.0\",\n",
      "        \"61.7\",\n",
      "        \"26.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"claude-instant\",\n",
      "        \"v1.1\",\n",
      "        \"1.60\",\n",
      "        \"16.7\",\n",
      "        \"18.0\",\n",
      "        \"20.8\",\n",
      "        \"5.9\",\n",
      "        \"12.6\",\n",
      "        \"30.0\",\n",
      "        \"49.7\",\n",
      "        \"4.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"chat-bison-001\",\n",
      "        \"\",\n",
      "        \"1.39\",\n",
      "        \"9.7\",\n",
      "        \"19.7\",\n",
      "        \"23.0\",\n",
      "        \"16.6\",\n",
      "        \"4.4\",\n",
      "        \"18.0\",\n",
      "        \"60.5\",\n",
      "        \"12.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"text-davinci-002\",\n",
      "        \"\",\n",
      "        \"1.25\",\n",
      "        \"8.3\",\n",
      "        \"16.7\",\n",
      "        \"41.5\",\n",
      "        \"11.8\",\n",
      "        \"0.5\",\n",
      "        \"16.0\",\n",
      "        \"56.3\",\n",
      "        \"9.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"OSS (Large)\",\n",
      "        \"llama-2-70b\",\n",
      "        \"chat\",\n",
      "        \"0.78\",\n",
      "        \"9.7\",\n",
      "        \"13.0\",\n",
      "        \"8.0\",\n",
      "        \"21.3\",\n",
      "        \"0.0\",\n",
      "        \"2.0\",\n",
      "        \"5.6\",\n",
      "        \"19.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"guanaco-65b\",\n",
      "        \"\",\n",
      "        \"0.54\",\n",
      "        \"8.3\",\n",
      "        \"14.7\",\n",
      "        \"1.9\",\n",
      "        \"0.1\",\n",
      "        \"1.5\",\n",
      "        \"12.0\",\n",
      "        \"0.9\",\n",
      "        \"10.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"codellama-34b instruct\",\n",
      "        \"\",\n",
      "        \"0.96\",\n",
      "        \"2.8\",\n",
      "        \"14.0\",\n",
      "        \"23.5\",\n",
      "        \"8.4\",\n",
      "        \"0.7\",\n",
      "        \"4.0\",\n",
      "        \"52.1\",\n",
      "        \"20.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"OSS (Medium)\",\n",
      "        \"vicuna-33b\",\n",
      "        \"v1.3\",\n",
      "        \"0.73\",\n",
      "        \"15.3\",\n",
      "        \"11.0\",\n",
      "        \"1.2\",\n",
      "        \"16.3\",\n",
      "        \"1.0\",\n",
      "        \"6.0\",\n",
      "        \"23.9\",\n",
      "        \"7.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"wizardlm-30b\",\n",
      "        \"v1.0\",\n",
      "        \"0.46\",\n",
      "        \"13.9\",\n",
      "        \"12.7\",\n",
      "        \"2.9\",\n",
      "        \"0.3\",\n",
      "        \"1.8\",\n",
      "        \"6.0\",\n",
      "        \"4.4\",\n",
      "        \"1.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"guanaco-33b\",\n",
      "        \"\",\n",
      "        \"0.39\",\n",
      "        \"11.1\",\n",
      "        \"9.3\",\n",
      "        \"3.2\",\n",
      "        \"0.3\",\n",
      "        \"0.0\",\n",
      "        \"6.0\",\n",
      "        \"6.2\",\n",
      "        \"5.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"vicuna-13b\",\n",
      "        \"v1.5\",\n",
      "        \"0.93\",\n",
      "        \"10.4\",\n",
      "        \"6.7\",\n",
      "        \"9.4\",\n",
      "        \"0.1\",\n",
      "        \"8.0\",\n",
      "        \"8.0\",\n",
      "        \"41.7\",\n",
      "        \"12.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"llama-2-13b\",\n",
      "        \"chat\",\n",
      "        \"0.77\",\n",
      "        \"4.2\",\n",
      "        \"11.7\",\n",
      "        \"3.6\",\n",
      "        \"26.4\",\n",
      "        \"0.0\",\n",
      "        \"6.0\",\n",
      "        \"25.3\",\n",
      "        \"13.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"openchat-13b\",\n",
      "        \"v3.2\",\n",
      "        \"0.70\",\n",
      "        \"15.3\",\n",
      "        \"12.3\",\n",
      "        \"5.5\",\n",
      "        \"0.1\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"46.9\",\n",
      "        \"15.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"wizardlm-13b\",\n",
      "        \"v1.2\",\n",
      "        \"0.66\",\n",
      "        \"9.0\",\n",
      "        \"12.7\",\n",
      "        \"1.7\",\n",
      "        \"1.9\",\n",
      "        \"0.0\",\n",
      "        \"10.0\",\n",
      "        \"43.7\",\n",
      "        \"12.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"vicuna-7b\",\n",
      "        \"v1.5\",\n",
      "        \"0.56\",\n",
      "        \"9.7\",\n",
      "        \"8.7\",\n",
      "        \"2.5\",\n",
      "        \"0.3\",\n",
      "        \"6.4\",\n",
      "        \"0.0\",\n",
      "        \"2.2\",\n",
      "        \"9.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"codellama-13b instruct\",\n",
      "        \"\",\n",
      "        \"0.56\",\n",
      "        \"3.5\",\n",
      "        \"9.7\",\n",
      "        \"10.4\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"43.8\",\n",
      "        \"14.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"OSS (Small)\",\n",
      "        \"codellama-7b instruct\",\n",
      "        \"\",\n",
      "        \"0.50\",\n",
      "        \"4.9\",\n",
      "        \"12.7\",\n",
      "        \"8.2\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"2.0\",\n",
      "        \"25.2\",\n",
      "        \"12.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"koala-13b\",\n",
      "        \"\",\n",
      "        \"0.34\",\n",
      "        \"3.5\",\n",
      "        \"5.0\",\n",
      "        \"0.4\",\n",
      "        \"0.1\",\n",
      "        \"4.4\",\n",
      "        \"0.0\",\n",
      "        \"3.9\",\n",
      "        \"7.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"llama-2-7b\",\n",
      "        \"chat\",\n",
      "        \"0.34\",\n",
      "        \"4.2\",\n",
      "        \"8.0\",\n",
      "        \"2.1\",\n",
      "        \"6.9\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"11.6\",\n",
      "        \"7.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"codegeex2-6b\",\n",
      "        \"\",\n",
      "        \"0.27\",\n",
      "        \"1.4\",\n",
      "        \"0.0\",\n",
      "        \"4.8\",\n",
      "        \"0.3\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"20.9\",\n",
      "        \"11.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"dolly-12b\",\n",
      "        \"v2\",\n",
      "        \"0.14\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.1\",\n",
      "        \"1.2\",\n",
      "        \"0.0\",\n",
      "        \"0.4\",\n",
      "        \"9.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"chatglm-6b\",\n",
      "        \"v1.1\",\n",
      "        \"0.11\",\n",
      "        \"4.9\",\n",
      "        \"0.3\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.5\",\n",
      "        \"4.9\"\n",
      "      ],\n",
      "      [\n",
      "        \"\",\n",
      "        \"oasst-12b\",\n",
      "        \"sft-4\",\n",
      "        \"0.03\",\n",
      "        \"1.4\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"0.3\",\n",
      "        \"1.0\"\n",
      "      ]\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"table_num\": 4,\n",
      "    \"title\": \"Portions of different types of execution\",\n",
      "    \"columns\": [\n",
      "      \"OS\",\n",
      "      \"DB\",\n",
      "      \"KG\",\n",
      "      \"DCG\",\n",
      "      \"LTP\",\n",
      "      \"HH\",\n",
      "      \"WS\",\n",
      "      \"WB\"\n",
      "    ],\n",
      "    \"rows\": [\n",
      "      [\n",
      "        \"Completed\",\n",
      "        \"75.0\",\n",
      "        \"37.9\",\n",
      "        \"30.1\",\n",
      "        \"51.2\",\n",
      "        \"14.0\",\n",
      "        \"13.1\",\n",
      "        \"54.9\",\n",
      "        \"56.6\"\n",
      "      ],\n",
      "      [\n",
      "        \"CLE\",\n",
      "        \"0.1\",\n",
      "        \"0.7\",\n",
      "        \"2.0\",\n",
      "        \"0.0\",\n",
      "        \"3.5\",\n",
      "        \"0.7\",\n",
      "        \"0.0\",\n",
      "        \"0.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"Invalid Format\",\n",
      "        \"0.0\",\n",
      "        \"53.3\",\n",
      "        \"0.0\",\n",
      "        \"38.5\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"17.2\",\n",
      "        \"0.0\"\n",
      "      ],\n",
      "      [\n",
      "        \"Invalid Action\",\n",
      "        \"0.9\",\n",
      "        \"0.0\",\n",
      "        \"0.0\",\n",
      "        \"10.2\",\n",
      "        \"0.0\",\n",
      "        \"64.1\",\n",
      "        \"0.0\",\n",
      "        \"8.4\"\n",
      "      ],\n",
      "      [\n",
      "        \"TLE\",\n",
      "        \"23.9\",\n",
      "        \"8.0\",\n",
      "        \"67.9\",\n",
      "        \"0.0\",\n",
      "        \"82.5\",\n",
      "        \"22.1\",\n",
      "        \"27.8\",\n",
      "        \"35.0\"\n",
      "      ]\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"table_num\": 5,\n",
      "    \"title\": \"Comparison of Outcome distribution for 2 types of models\",\n",
      "    \"columns\": [\n",
      "      \"Model Category\",\n",
      "      \"Completed\",\n",
      "      \"Context Limit Exceeded\",\n",
      "      \"Invalid Format\",\n",
      "      \"Invalid Action\",\n",
      "      \"Task Limit Exceeded\"\n",
      "    ],\n",
      "    \"rows\": [\n",
      "      [\n",
      "        \"Commercial API-based Models\",\n",
      "        \"61.5%\",\n",
      "        \"3.0%\",\n",
      "        \"6.0%\",\n",
      "        \"4.6%\",\n",
      "        \"24.9%\"\n",
      "      ],\n",
      "      [\n",
      "        \"Open-Sourced Models\",\n",
      "        \"39.1%\",\n",
      "        \"0.0%\",\n",
      "        \"10.4%\",\n",
      "        \"13.6%\",\n",
      "        \"36.9%\"\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "doc_url = \"https://arxiv.org/pdf/2308.03688\"\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(doc_url).content\n",
    "\n",
    "prompt = \"Extract all the tables from this paper as json objects.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sample_image_from_paper_agent_bench.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of the figure, explaining the two plots and the overall message:\n",
      "\n",
      "**Figure 1: Overview of LLMs on AgentBench**\n",
      "\n",
      "The figure presents the performance of various Large Language Models (LLMs) on the AgentBench benchmark.  AgentBench is a tool designed to evaluate how well LLMs perform as agents in different simulated environments.  The key takeaway is that there are significant differences in performance, indicating that the gap towards practical usability is still considerable.\n",
      "\n",
      "**Panel (a):  Typical LLMs' AgentBench Performance (Relative)**\n",
      "\n",
      "*   **Chart Type:**  Radar chart\n",
      "*   **What it shows:**  This shows the *relative* performance of several LLMs across 8 different environment categories within the AgentBench framework. The environments are:\n",
      "    *   Operating System\n",
      "    *   Database\n",
      "    *   Knowledge Graph\n",
      "    *   Digital Card Game\n",
      "    *   Lateral Thinking Puzzle\n",
      "    *   House-holding\n",
      "    *   Web Shopping\n",
      "    *   Web Browsing\n",
      "\n",
      "*   **How to interpret:**  The further a line extends toward the edge of the chart for a particular environment, the better the model performs in that environment *relative* to the best-performing model for that specific environment.  This means that if gpt-4 reaches the outermost edge of the \"Operating System\" axis, then it is currently the best LLM for that task. It provides a visual comparison of strengths and weaknesses across different tasks.\n",
      "\n",
      "*   **Key Observations:**\n",
      "    *   `gpt-4` appears to have the most extensive coverage.\n",
      "    *   Other models such as `claude-2` and `gpt-3.5-turbo (0613)` also show relatively high performance, but are more spiky in the graph which means their performance is more variable across the different environments.\n",
      "\n",
      "**Panel (b): Overall Scores of AgentBench Across 8 Environments**\n",
      "\n",
      "*   **Chart Type:**  Horizontal bar chart\n",
      "*   **What it shows:**  The *overall* AgentBench score for each listed LLM, averaged across the 8 different environments.\n",
      "\n",
      "*   **Interpretation:** The x-axis represents the AgentBench Overall Score.  Longer bars mean higher average performance across all the environments tested.\n",
      "    *   The bars are divided into two groups based on LLM type: API-based Commercial LLMs (in gold), and OSS LLMs (Open Source LLMs, in light green)\n",
      "    *   There are vertical dashed lines representing the average score for each type of LLM.\n",
      "\n",
      "*   **Key Observations:**\n",
      "\n",
      "    *   **Performance Hierarchy:**  The API-based Commercial LLMs (like `gpt-4`, `claude-2`, and `gpt-3.5-turbo`) consistently outperform the OSS LLMs.\n",
      "    *   **Average Scores:** The average performance of API-based Commercial LLMs (Avg: 2.15) is much higher than OSS LLMs (Avg: 0.51), demonstrating a considerable performance gap.\n",
      "\n",
      "**Overall Message**\n",
      "\n",
      "The figure illustrates that while LLMs are increasingly capable as agents, there are significant differences in performance, particularly between commercial API-based models and open-source models. The gap in performance between models and the levels needed for practical application remains significant. The radar chart highlights the varying strengths of different models across different environment types. The bar chart indicates the clear advantage commercial LLMs currently have in overall performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "my_file = client.files.upload(file=\"./sample_image_from_paper_agent_bench.png\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[my_file, f\"Explain this figure.\"],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
