{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you read this???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AI, or Artificial Intelligence, broadly encompasses computer systems mimicking human intelligence. Currently, most AI relies on **machine learning**, particularly **deep learning**.\n",
       "\n",
       "Machine learning involves feeding algorithms massive datasets. The algorithm then identifies patterns and relationships within this data, allowing it to make predictions or decisions on new, unseen data.\n",
       "\n",
       "Deep learning uses artificial neural networks with multiple layers to analyze data in a hierarchical way, similar to how the human brain processes information. These networks learn complex features from raw data like images, text, or sound.\n",
       "\n",
       "For example, to recognize cats in images, a deep learning model is trained on millions of cat pictures. It learns to identify edges, textures, and shapes that define a cat, allowing it to accurately identify cats in new images.\n",
       "\n",
       "While impressive, AI is not truly \"thinking\" like humans. It's sophisticated pattern recognition and application, reliant on the quality and quantity of data it's trained on. Different AI approaches exist beyond machine learning, but this is the dominant method today.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in 200 words.\"\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini for Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets-resources/sample-image.png\" width=20%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the bounding box detections:\n",
      "```json\n",
      "[\n",
      "  {\"box_2d\": [563, 276, 579, 312], \"label\": \"Table\"},\n",
      "  {\"box_2d\": [896, 237, 942, 937], \"label\": \"(illustrated by tokens of different colors in the input sequence). They can output responses with\\ninterleaved image and text.\"},\n",
      "  {\"box_2d\": [725, 792, 780, 839], \"label\": \"Aa\"},\n",
      "  {\"box_2d\": [877, 236, 896, 292], \"label\": \"Figure\"},\n",
      "  {\"box_2d\": [877, 301, 896, 309], \"label\": \"2\"},\n",
      "  {\"box_2d\": [616, 289, 637, 310], \"label\": \"Aa\"},\n",
      "  {\"box_2d\": [580, 276, 595, 325], \"label\": \"Sequence\"},\n",
      "  {\"box_2d\": [664, 528, 782, 630], \"label\": \"Transformer\"},\n",
      "  {\"box_2d\": [877, 315, 896, 916], \"label\": \"Gemini models support interleaved sequences of text, image, audio, and video as inputs\"},\n",
      "  {\"box_2d\": [343, 58, 382, 173], \"label\": \"arrow\"},\n",
      "  {\"box_2d\": [746, 719, 758, 757], \"label\": \"Decoder\"}\n",
      "]\n",
      "```\n",
      "The image presents a diagram of the Gemini 1.0 model architecture. It illustrates how the model is structured to handle various types of input, including text, images, audio, and video. The model builds on top of Transformer decoders and utilizes efficient attention mechanisms to support 32k context length. The diagram shows how these inputs are processed through a Transformer and then decoded into image and text outputs. The caption emphasizes that the Gemini models support interleaved sequences of these inputs and can produce responses with interleaved image and text.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "my_file = client.files.upload(file=\"./assets-resources/sample-image.png\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[my_file, \"Caption this image.\"],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To pass image data inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 illustrates how Gemini models support interleaved sequences of text, image, audio, and video as inputs and can output interleaved image and text responses.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "with open('./assets-resources/sample-image.png', 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model='gemini-2.5-flash-preview-04-17',\n",
    "contents=[\n",
    "    types.Part.from_bytes(\n",
    "    data=image_bytes,\n",
    "    mime_type='image/png',\n",
    "    ),\n",
    "    'What is the diagram in this picture?Answer it concisely.'\n",
    "]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more examples for working with images in Gemini [here](https://ai.google.dev/gemini-api/docs/image-understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a bullet-point summary of the document:\n",
      "\n",
      "*   **Problem:** Predicting a protein's 3D structure from its amino acid sequence is crucial for understanding function but experimentally difficult and time-consuming.\n",
      "*   **Background:** Previous methods used genetic covariation analysis to infer residue contacts, aiding structure prediction. Fragment assembly methods used statistical potentials and sampling.\n",
      "*   **AlphaFold's Novel Approach:**\n",
      "    *   Trains a deep neural network to predict accurate *distances* between pairs of residues (specifically, the Cβ atoms). Distance predictions provide richer structural information than binary contact predictions.\n",
      "    *   Constructs a protein-specific potential of mean force based on these predicted distance distributions.\n",
      "    *   Optimizes this potential using a simple gradient descent algorithm (L-BFGS) to realize protein structures, avoiding complex sampling procedures.\n",
      "    *   Predicts full chains without requiring explicit domain segmentation.\n",
      "*   **Neural Network Details:**\n",
      "    *   Input: Amino acid sequence and features derived from multiple sequence alignments (MSAs).\n",
      "    *   Architecture: Deep two-dimensional dilated convolutional residual network.\n",
      "    *   Output: A \"distogram\" – a discrete probability distribution over distance bins (2-22 Å, 64 bins) for each residue pair.\n",
      "    *   Also predicts secondary structure and torsion angle distributions.\n",
      "    *   Trained using crops of the distance matrix and data augmentation (MSA subsampling, coordinate noise).\n",
      "*   **Potential Construction:** Combines terms derived from the predicted distance distributions, predicted torsion angle distributions, and a steric clash term (based on Rosetta's Vscore2_smooth). A reference distribution is subtracted from the distance term. The potential is differentiable with respect to backbone torsion angles.\n",
      "*   **Structure Realization:** Minimizes the potential using gradient descent. Uses multiple initializations, including \"noisy restarts\", to explore the conformational space.\n",
      "*   **CASP13 Performance:**\n",
      "    *   AlphaFold achieved high accuracy in the CASP13 blind assessment (2018).\n",
      "    *   Significantly outperformed other methods, particularly in the challenging Free Modelling (FM) category.\n",
      "    *   Achieved TM-scores of 0.7 or higher for 24 out of 43 FM domains, compared to 14 for the next best method.\n",
      "    *   Ranked highly in combined FM and Template-Based Modelling (TBM)/FM categories.\n",
      "    *   Demonstrated high precision in long-range contact predictions (Table 1c), derived from the accurate distance predictions.\n",
      "    *   Accuracy correlates strongly with the accuracy of the predicted distogram (Figure 4a).\n",
      "*   **Biological Relevance & Impact:**\n",
      "    *   Increased accuracy provides better insights into protein function and malfunction, especially for proteins without experimental structures.\n",
      "    *   Improves structure-based searches for homologous proteins (Figure 11).\n",
      "    *   Leads to more accurate prediction of protein-protein interaction interfaces, potentially aiding docking (Figure 12).\n",
      "    *   Improves prediction of ligand binding pocket geometry (Figure 13).\n",
      "    *   Predicted structures can assist in experimental methods like X-ray crystallography (molecular replacement).\n",
      "*   **Model Interpretation:** Attribution analysis (Integrated Gradients) reveals that the network leverages spatially structured information from input features, reflecting protein geometry and secondary structure interactions, to make distance predictions (Figures 14, 15).\n",
      "*   **Availability:** Code for distogram/torsion predictions, weights, and CASP13 input data will be made available upon publication. Uses publicly available datasets (PDB, CATH, Uniclust) and open-source libraries (TensorFlow, Sonnet, HHblits, PSI-BLAST).\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(doc_url).content\n",
    "\n",
    "prompt = \"Summarize this document in bullet points\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-flash-preview-04-17\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For locally stored pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is a 3-page markdown report summarizing the practical tips and relevant information from the Google Prompt Engineering guide.\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 1/3)\n",
      "\n",
      "## 1. Introduction to Prompt Engineering\n",
      "\n",
      "*   **Core Idea:** Prompt engineering is the iterative process of designing effective inputs (prompts) to guide Large Language Models (LLMs) toward desired outputs. It's essential because LLMs are prediction engines, and the prompt sets the context for that prediction.\n",
      "*   **Accessibility:** You don't need to be a data scientist; anyone can write prompts, but crafting *effective* ones takes practice and iteration.\n",
      "*   **Goal:** To create prompts that are clear, specific, and provide sufficient context, leading to accurate, relevant, and useful LLM responses. Inadequate prompts cause ambiguity and poor results.\n",
      "*   **Scope:** This guide focuses on prompting models like Gemini directly (via API or tools like Vertex AI Studio) where configuration is accessible.\n",
      "\n",
      "## 2. Essential LLM Output Configuration\n",
      "\n",
      "*Before* focusing solely on the prompt text, configure the model's output parameters. These significantly impact the results:\n",
      "\n",
      "*   **Output Length (Max Tokens):**\n",
      "    *   Sets the maximum number of tokens the model will generate.\n",
      "    *   **Practical Tip:** Be mindful of costs, latency, and energy use (more tokens = higher). Don't rely on this alone for succinctness; adjust the prompt too. Crucial for techniques like ReAct to prevent excessive output. Too short can truncate output (e.g., invalid JSON).\n",
      "*   **Sampling Controls (Temperature, Top-K, Top-P):** These control the randomness and creativity of the output.\n",
      "    *   **Temperature:**\n",
      "        *   Controls randomness. Lower values (~0.1-0.3) = more deterministic, focused, factual. Higher values (~0.7-1.0) = more creative, diverse, potentially unexpected.\n",
      "        *   **Practical Tip:** Use `0` for tasks with a single correct answer (math, strict data extraction). Start around `0.2` for factual but slightly flexible tasks, and `0.7-0.9` for creative tasks. Be wary of very high temps causing incoherence or the \"repetition loop bug\".\n",
      "    *   **Top-K:**\n",
      "        *   Considers only the `K` most likely next tokens. Lower `K` = more restricted/conservative. Higher `K` = more diverse. `K=1` is deterministic (like Temp 0).\n",
      "        *   **Practical Tip:** Start around `30-40`. Lower `K` (~20) for more factual, higher `K` (~40+) for creative.\n",
      "    *   **Top-P (Nucleus Sampling):**\n",
      "        *   Considers the smallest set of tokens whose cumulative probability exceeds `P`. Lower `P` = more conservative. Higher `P` (~0.95-1.0) = more diverse. `P=0` (or very small) often defaults to the single most likely token. `P=1` considers all tokens.\n",
      "        *   **Practical Tip:** Often used *instead* of or *with* Top-K. A common starting point is `0.95`. Lower `P` (~0.9) for factual, higher `P` (~0.99) for creative.\n",
      "    *   **Putting it Together:** The model typically filters by Top-K and Top-P first, then applies Temperature to the remaining candidates. Extreme settings in one can make others irrelevant (e.g., Temp 0 ignores K/P; K=1 ignores Temp/P).\n",
      "    *   **Starting Point Recommendation:** Temp `0.2`, Top-P `0.95`, Top-K `30` for balanced results. Adjust based on desired creativity/factuality.\n",
      "\n",
      "## 3. Foundational Prompting Techniques\n",
      "\n",
      "*   **Zero-Shot Prompting:**\n",
      "    *   Provide only the task description or question without any examples.\n",
      "    *   `Example: Classify the following movie review: [Review Text]`\n",
      "    *   **Practical Tip:** Simplest method, good starting point. May fail for complex tasks or when specific output formats are needed.\n",
      "*   **One-Shot / Few-Shot Prompting:**\n",
      "    *   Provide one (one-shot) or multiple (few-shot) examples of the task and desired output.\n",
      "    *   `Example (Few-Shot Sentiment):`\n",
      "        `Review: \"Loved it!\" Sentiment: Positive`\n",
      "        `Review: \"Boring.\" Sentiment: Negative`\n",
      "        `Review: \"It was okay.\" Sentiment: Neutral`\n",
      "        `Review: \"[New Review Text]\" Sentiment:`\n",
      "    *   **Practical Tip:** Highly effective for guiding the model on structure, style, and task logic. Use 3-5 high-quality, diverse examples as a rule of thumb. Include edge cases if needed. Ensure examples are accurate, as errors confuse the model.\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 2/3)\n",
      "\n",
      "## 4. Intermediate Prompting Techniques\n",
      "\n",
      "*   **System, Contextual, and Role Prompting:**\n",
      "    *   **System Prompt:** Defines the overall task, fundamental purpose, or constraints (e.g., \"Translate the following text to French.\", \"Only return JSON.\").\n",
      "    *   **Contextual Prompt:** Provides specific background information relevant to the *current* task or query (e.g., \"Given the previous conversation about user preferences, suggest a suitable product.\").\n",
      "    *   **Role Prompt:** Assigns a persona or identity to the LLM (e.g., \"Act as a pirate.\", \"You are a helpful travel guide specialized in budget travel.\").\n",
      "    *   **Practical Tip:** Use **Role Prompting** to control tone, style, and expertise (e.g., \"Explain this concept like I'm five.\", \"Write in a formal, academic style.\"). Combine these types as needed (e.g., a Role prompt can include Context).\n",
      "*   **Step-Back Prompting:**\n",
      "    *   Ask the LLM a more general, abstract question related to the specific task *first*. Then, use the answer to that general question as context when asking the specific task prompt.\n",
      "    *   **Practical Tip:** Improves reasoning by activating broader knowledge. Useful for complex problems or mitigating bias. Requires two LLM calls.\n",
      "*   **Chain of Thought (CoT) Prompting:**\n",
      "    *   Instruct the LLM to break down its reasoning process step-by-step before giving the final answer. Simply add phrases like \"Let's think step by step.\"\n",
      "    *   `Example: Q: [Math Problem]. Let's think step by step. A:`\n",
      "    *   **Practical Tip:** Significantly improves performance on tasks requiring reasoning (math, logic puzzles). Provides interpretability. Works well combined with few-shot examples showing the reasoning steps. Use **Temperature 0** for CoT tasks. Ensure the final answer comes *after* the reasoning steps. More tokens = higher cost/latency.\n",
      "*   **Self-Consistency:**\n",
      "    *   An enhancement to CoT. Run the same CoT prompt multiple times with a higher temperature (to generate diverse reasoning paths). Select the most frequent final answer (majority vote).\n",
      "    *   **Practical Tip:** Improves accuracy over basic CoT, especially for complex reasoning. Significantly increases cost due to multiple runs.\n",
      "*   **Tree of Thoughts (ToT):**\n",
      "    *   (Advanced) Explores multiple reasoning paths simultaneously, forming a tree structure. Better for complex exploration tasks. Less common in basic prompt engineering.\n",
      "*   **ReAct (Reason + Act):**\n",
      "    *   Enables LLMs to use external tools (like search APIs, code interpreters) by interleaving reasoning steps (`Thought:`) with actions (`Action:`, `Action Input:`) and observing results (`Observation:`).\n",
      "    *   **Practical Tip:** Foundational for building agents. Requires external frameworks (e.g., LangChain) and tool setup (API keys). Needs careful management of the prompt history (context) sent back to the LLM in each step. Restrict output length to avoid runaway actions.\n",
      "*   **Automatic Prompt Engineering (APE):**\n",
      "    *   Use an LLM to generate variations of an initial prompt for a specific task. Evaluate these generated prompts (manually or using metrics like BLEU/ROUGE) and select the best one.\n",
      "    *   **Practical Tip:** Can help discover effective prompt phrasing, especially for training data generation. Iterative process.\n",
      "\n",
      "## 5. Code Prompting Specifics\n",
      "\n",
      "*   LLMs like Gemini can understand and generate code.\n",
      "*   **Use Cases:**\n",
      "    *   **Writing Code:** Provide a description of the desired functionality. (e.g., \"Write a Python script to rename files in a folder, prepending 'draft_'\").\n",
      "    *   **Explaining Code:** Paste code and ask for an explanation. (e.g., \"Explain this Bash script line by line.\").\n",
      "    *   **Translating Code:** Provide code in one language and ask for another. (e.g., \"Translate this Bash script to Python.\").\n",
      "    *   **Debugging & Reviewing Code:** Provide code and the error message, ask for debugging help, or ask for general improvements/review.\n",
      "*   **Practical Tips:**\n",
      "    *   **ALWAYS TEST GENERATED CODE.** LLMs can make subtle or significant errors.\n",
      "    *   Be specific about the language, libraries, and desired functionality.\n",
      "    *   For debugging, provide the full error message and relevant code snippet.\n",
      "    *   In tools like Vertex AI Studio, use the 'Markdown' view for code output to preserve formatting (especially Python indentation).\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 3/3)\n",
      "\n",
      "## 6. Best Practices for Effective Prompting\n",
      "\n",
      "*   **Provide Examples (Few-Shot):** (Reiteration) Often the single most effective technique. Show, don't just tell.\n",
      "*   **Design with Simplicity:** Clear, concise language. Avoid jargon or unnecessary info. If it's confusing to you, it's likely confusing to the model.\n",
      "    *   **Tip:** Use clear action verbs (e.g., `Summarize`, `Classify`, `Generate`, `Translate`, `Extract`, `Rewrite`).\n",
      "*   **Be Specific About the Output:** Clearly define the desired format, length, style, content, and target audience. Don't be vague (e.g., \"Write a 3-paragraph blog post for beginners...\" vs. \"Write about consoles.\").\n",
      "*   **Use Instructions over Constraints:** Tell the model *what to do* rather than only *what not to do*. Constraints are okay for safety guardrails or strict formatting but can be less effective or conflicting.\n",
      "    *   `DO: Summarize the text in 3 bullet points.`\n",
      "    *   `LESS EFFECTIVE: Do not write a long summary. Do not use paragraphs.`\n",
      "*   **Control Max Token Length:** Use configuration or specify length in the prompt (e.g., \"...in under 100 words,\" \"...in a single sentence\").\n",
      "*   **Use Variables in Prompts:** Use placeholders (like `{city}` or `$user_input`) to make prompts reusable and dynamic. Essential for integrating prompts into applications.\n",
      "*   **Experiment Iteratively:** Try different phrasing, formats (question vs. instruction), styles, examples, configurations, and even different models/versions. Prompt engineering is not a one-shot process.\n",
      "*   **Mix Classes (Few-Shot Classification):** When providing examples for classification, ensure the examples cover different classes and aren't all clustered together to avoid order bias.\n",
      "*   **Adapt to Model Updates:** Newer model versions may have different capabilities or respond differently. Re-test prompts with new versions.\n",
      "*   **Experiment with Output Formats (JSON/XML):**\n",
      "    *   For non-creative tasks (extraction, classification, structured data), explicitly ask for output in JSON or XML.\n",
      "    *   **Benefits:** Consistent structure, easier parsing in applications, can enforce data types, reduces hallucination likelihood.\n",
      "    *   **Tip:** Provide the desired schema or an example JSON structure in the prompt (few-shot). Be mindful of token limits, as JSON is verbose. Use tools like the `json-repair` library (Python) to fix truncated/malformed JSON output.\n",
      "*   **Working with Schemas (Input):** Provide a JSON Schema definition along with the JSON input data. This helps the LLM understand the structure and focus on relevant fields, especially for complex or large inputs.\n",
      "*   **Collaborate:** If possible, have multiple people attempt prompt design and compare results.\n",
      "*   **DOCUMENT EVERYTHING:**\n",
      "    *   **Crucial:** Keep detailed records of your prompt attempts.\n",
      "    *   **Template Fields:** Prompt Name/Version, Goal, Model Used, Temperature, Top-K, Top-P, Max Tokens, Full Prompt Text, Output(s), Outcome (OK/Not OK/Sometimes OK), Feedback/Notes, Hyperlink (if saved in a tool like Vertex AI Studio).\n",
      "    *   **Why:** Enables learning, debugging, re-testing on new models, and avoids re-doing work.\n",
      "    *   **Tip:** Store prompts in separate files from application code for maintainability. Consider automated testing/evaluation for prompts in production.\n",
      "\n",
      "## 7. Final Takeaway\n",
      "\n",
      "Effective prompt engineering is an iterative cycle: **Craft -> Test -> Analyze -> Document -> Refine.** It requires understanding the LLM's configuration options, leveraging different prompting techniques (especially examples), clearly stating intent, and meticulously documenting experiments to achieve consistent, high-quality results.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "# Uncomment this to download from the internet and save it locally\n",
    "# doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "# filepath = pathlib.Path('./assets-resources/prompt-eng-guide-google.pdf')\n",
    "# filepath.write_bytes(httpx.get(doc_url).content)\n",
    "\n",
    "# This example assumes the pdf is stored locally\n",
    "# Retrieve and encode the PDF byte\n",
    "filepath = pathlib.Path('./assets-resources/prompt-eng-guide-google.pdf')\n",
    "\n",
    "prompt = \"Write markdown style 3 page report on this prompt engineering guide with just the practical tips and relevant information.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-pro-preview-03-25\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live coding sesh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can germini 2.5 flash be used to pull all tables, of different format, in  a pdf, say of around 100 pages? or is there a more efficient way, model or steps you could recommend? Some table have lines, others do not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./table1-paper.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "| LLM Type   | Model              | #Size | Form          | Ver.   | Creator               |\n",
      "| :--------- | :----------------- | :---- | :------------ | :----- | :-------------------- |\n",
      "| API        | gpt-4              | N/A   | api           | 0613   | OpenAI                |\n",
      "| API        | gpt-3.5-turbo      | N/A   | api           | 0613   | OpenAI                |\n",
      "| API        | text-davinci-003   | N/A   | api           |        | OpenAI                |\n",
      "| API        | text-davinci-002   | N/A   | api           |        | OpenAI                |\n",
      "| API        | claude-2           | N/A   | api           |        | Anthropic             |\n",
      "| API        | claude             | N/A   | api           | v1.3   | Anthropic             |\n",
      "| API        | claude-instant     | N/A   | api           | v1.1   | Anthropic             |\n",
      "| API        | chat-bison-001     | N/A   | api           |        | Google                |\n",
      "| OSS        | codellama-34b      | 34B   | open instruct |        | Meta                  |\n",
      "| OSS        | codellama-13b      | 13B   | open instruct |        | Meta                  |\n",
      "| OSS        | codellama-7b       | 7B    | open instruct |        | Meta                  |\n",
      "| OSS        | dolly-12b          | 12B   | open          | v2     | Databricks            |\n",
      "| OSS        | llama2-70b         | 70B   | open chat     |        | Meta                  |\n",
      "| OSS        | llama2-13b         | 13B   | open chat     |        | Meta                  |\n",
      "| OSS        | llama2-7b          | 7B    | open chat     |        | Meta                  |\n",
      "| OSS        | guanaco-65b        | 65B   | open          |        | Meta                  |\n",
      "| OSS        | guanaco-33b        | 33B   | open          | v1.3   |                       |\n",
      "| OSS        | vicuna-33b         | 33B   | open          | v1.3   | LMSYS                 |\n",
      "| OSS        | vicuna-13b         | 13B   | open          | v1.5   | LMSYS                 |\n",
      "| OSS        | vicuna-7b          | 7B    | open          | v1.5   | LMSYS                 |\n",
      "| OSS        | chatglm-6b         | 6B    | open          | v1.1   | Tsinghua & Zhipu      |\n",
      "| OSS        | codegeex2-6b       | 6B    | open          |        | Microsoft             |\n",
      "| OSS        | openchat-13b       | 13B   | open          | v3.2   | UCB                   |\n",
      "| OSS        | wizardlm-30b       | 30B   | open          | v1.0   |                       |\n",
      "| OSS        | wizardlm-13b       | 13B   | open          | v1.0   |                       |\n",
      "| OSS        | koala-13b          | 13B   | open          |        | UCB                   |\n",
      "| OSS        | oasst-12b          | 12B   | open sft-4    |        | LAION                 |\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "pdf_path = \"./paper.pdf\"\n",
    "\n",
    "filepath = pathlib.Path(pdf_path)\n",
    "\n",
    "prompt = \"Extract table 1 from this paper. Your output should be just the table. Make it markdown style.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-flash-preview-04-17\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      " \"Table 1: AGENTBENCH evaluates 27 API-based or OSS LLMs on LLM-as-Agent challenges.\": {\n",
      "  \"Model\": [\n",
      "   \"gpt-4 (OpenAI, 2023)\",\n",
      "   \"gpt-3.5-turbo (OpenAI, 2022)\",\n",
      "   \"text-davinci-003 (Ouyang et al., 2022)\",\n",
      "   \"text-davinci-002 (Ouyang et al., 2022)\",\n",
      "   \"claude-2 (Anthropic, 2023b)\",\n",
      "   \"claude (Anthropic, 2023a)\",\n",
      "   \"claude-instant (Anthropic, 2023a)\",\n",
      "   \"chat-bison-001 (Anil et al., 2023)\",\n",
      "   \"chatglm-6b (Zeng et al., 2022; Du et al., 2022)\",\n",
      "   \"codegeex2-6b (Zheng et al., 2023)\",\n",
      "   \"codellama-34b (Rozière et al., 2023)\",\n",
      "   \"codellama-13b (Rozière et al., 2023)\",\n",
      "   \"codellama-7b (Rozière et al., 2023)\",\n",
      "   \"dolly-12b (Conover et al., 2023)\",\n",
      "   \"llama2-70b (Touvron et al., 2023)\",\n",
      "   \"llama2-13b (Touvron et al., 2023)\",\n",
      "   \"llama2-7b (Touvron et al., 2023)\",\n",
      "   \"guanaco-65b (Dettmers et al., 2023)\",\n",
      "   \"guanaco-33b (Dettmers et al., 2023)\",\n",
      "   \"vicuna-33b (Chiang et al., 2023)\",\n",
      "   \"vicuna-13b (Chiang et al., 2023)\",\n",
      "   \"vicuna-7b (Chiang et al., 2023)\",\n",
      "   \"openchat-13b (Wang et al., 2023a)\",\n",
      "   \"wizardlm-30b (Xu et al., 2023)\",\n",
      "   \"wizardlm-13b (Xu et al., 2023)\",\n",
      "   \"koala-13b (Geng et al., 2023)\",\n",
      "   \"oasst-12b (LAION, 2023)\"\n",
      "  ],\n",
      "  \"#Size\": [\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"N/A\",\n",
      "   \"6B\",\n",
      "   \"6B\",\n",
      "   \"34B\",\n",
      "   \"13B\",\n",
      "   \"7B\",\n",
      "   \"12B\",\n",
      "   \"70B\",\n",
      "   \"13B\",\n",
      "   \"7B\",\n",
      "   \"65B\",\n",
      "   \"33B\",\n",
      "   \"33B\",\n",
      "   \"13B\",\n",
      "   \"7B\",\n",
      "   \"13B\",\n",
      "   \"30B\",\n",
      "   \"13B\",\n",
      "   \"13B\",\n",
      "   \"12B\"\n",
      "  ],\n",
      "  \"Form\": [\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"api\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\",\n",
      "   \"open\"\n",
      "  ],\n",
      "  \"Ver.\": [\n",
      "   \"0613\",\n",
      "   \"0613\",\n",
      "   \"-\",\n",
      "   \"-\",\n",
      "   \"-\",\n",
      "   \"v1.3\",\n",
      "   \"v1.1\",\n",
      "   \"-\",\n",
      "   \"v1.1\",\n",
      "   \"-\",\n",
      "   \"instruct\",\n",
      "   \"instruct\",\n",
      "   \"instruct\",\n",
      "   \"v2\",\n",
      "   \"chat\",\n",
      "   \"chat\",\n",
      "   \"chat\",\n",
      "   \"-\",\n",
      "   \"-\",\n",
      "   \"v1.3\",\n",
      "   \"v1.5\",\n",
      "   \"v1.5\",\n",
      "   \"v3.2\",\n",
      "   \"v1.0\",\n",
      "   \"v1.0\",\n",
      "   \"-\",\n",
      "   \"sft-4\"\n",
      "  ],\n",
      "  \"Creator\": [\n",
      "   \"OpenAI\",\n",
      "   \"OpenAI\",\n",
      "   \"OpenAI\",\n",
      "   \"OpenAI\",\n",
      "   \"Anthropic\",\n",
      "   \"Anthropic\",\n",
      "   \"Anthropic\",\n",
      "   \"Google\",\n",
      "   \"Tsinghua\",\n",
      "   \"& Zhipu\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"Databricks\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"Meta\",\n",
      "   \"LMSYS\",\n",
      "   \"LMSYS\",\n",
      "   \"LMSYS\",\n",
      "   \"Tsinghua\",\n",
      "   \"Microsoft\",\n",
      "   \"Microsoft\",\n",
      "   \"UCB\",\n",
      "   \"LAION\"\n",
      "  ]\n",
      " }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "pdf_figure_image_path = \"./table1-paper.png\"\n",
    "\n",
    "with open(pdf_figure_image_path, 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model='gemini-2.5-flash-preview-04-17',\n",
    "contents=[\n",
    "    types.Part.from_bytes(\n",
    "    data=image_bytes,\n",
    "    mime_type='image/png',\n",
    "    ),\n",
    "    'Extract the full table in this example as a dictionary.'\n",
    "]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table 1: AGENTBENCH evaluates 27 API-based or OSS LLMs on LLM-as-Agent challenges.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>[gpt-4 (OpenAI, 2023), gpt-3.5-turbo (OpenAI, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Size</th>\n",
       "      <td>[N/A, N/A, N/A, N/A, N/A, N/A, N/A, N/A, 6B, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Form</th>\n",
       "      <td>[api, api, api, api, api, api, api, api, open,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ver.</th>\n",
       "      <td>[0613, 0613, -, -, -, v1.3, v1.1, -, v1.1, -, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Creator</th>\n",
       "      <td>[OpenAI, OpenAI, OpenAI, OpenAI, Anthropic, An...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Table 1: AGENTBENCH evaluates 27 API-based or OSS LLMs on LLM-as-Agent challenges.\n",
       "Model    [gpt-4 (OpenAI, 2023), gpt-3.5-turbo (OpenAI, ...                                \n",
       "#Size    [N/A, N/A, N/A, N/A, N/A, N/A, N/A, N/A, 6B, 6...                                \n",
       "Form     [api, api, api, api, api, api, api, api, open,...                                \n",
       "Ver.     [0613, 0613, -, -, -, v1.3, v1.1, -, v1.1, -, ...                                \n",
       "Creator  [OpenAI, OpenAI, OpenAI, OpenAI, Anthropic, An...                                "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def extract_table_to_dataframe(json_str):\n",
    "    \"\"\"\n",
    "    Convert the JSON table output from Gemini into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        json_str (str): JSON string containing the table data, expected to be wrapped in ```json\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data\n",
    "    \"\"\"\n",
    "    # Clean the string by removing ```json and ``` if present\n",
    "    cleaned_str = json_str.strip()\n",
    "    if cleaned_str.startswith('```json'):\n",
    "        cleaned_str = cleaned_str[7:]\n",
    "    if cleaned_str.endswith('```'):\n",
    "        cleaned_str = cleaned_str[:-3]\n",
    "    cleaned_str = cleaned_str.strip()\n",
    "    \n",
    "    # Parse the JSON string into a Python dictionary\n",
    "    data = json.loads(cleaned_str)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = extract_table_to_dataframe(response.text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Schema\nitems.properties.table_contents.additionalProperties\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtract the table 1 from this paper:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Example usage with the previous response\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m table_contents \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mgenerate_content(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgemini-2.5-flash-preview-04-17\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         contents\u001b[39m=\u001b[39;49m[types\u001b[39m.\u001b[39;49mPart\u001b[39m.\u001b[39;49mfrom_bytes(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         data\u001b[39m=\u001b[39;49mfilepath\u001b[39m.\u001b[39;49mread_bytes(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         mime_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mapplication/pdf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m       ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m       prompt],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         config\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mresponse_mime_type\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mapplication/json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mresponse_schema\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mlist\u001b[39;49m[Table],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/gemini-intro/notebooks/1.0-gemini-api-intro.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m table_contents\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/google/genai/models.py:5019\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5017\u001b[0m \u001b[39mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   5018\u001b[0m   i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 5019\u001b[0m   response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_content(\n\u001b[1;32m   5020\u001b[0m       model\u001b[39m=\u001b[39;49mmodel, contents\u001b[39m=\u001b[39;49mcontents, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m   5021\u001b[0m   )\n\u001b[1;32m   5022\u001b[0m   logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAFC remote call \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m is done.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   5023\u001b[0m   remaining_remote_calls_afc \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/google/genai/models.py:3971\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3969\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{model}\u001b[39;00m\u001b[39m:generateContent\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3970\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3971\u001b[0m   request_dict \u001b[39m=\u001b[39m _GenerateContentParameters_to_mldev(\n\u001b[1;32m   3972\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_api_client, parameter_model\n\u001b[1;32m   3973\u001b[0m   )\n\u001b[1;32m   3974\u001b[0m   request_url_dict \u001b[39m=\u001b[39m request_dict\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_url\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3975\u001b[0m   \u001b[39mif\u001b[39;00m request_url_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/google/genai/models.py:632\u001b[0m, in \u001b[0;36m_GenerateContentParameters_to_mldev\u001b[0;34m(api_client, from_object, parent_object)\u001b[0m\n\u001b[1;32m    617\u001b[0m   setv(\n\u001b[1;32m    618\u001b[0m       to_object,\n\u001b[1;32m    619\u001b[0m       [\u001b[39m'\u001b[39m\u001b[39mcontents\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m       ],\n\u001b[1;32m    626\u001b[0m   )\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m getv(from_object, [\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m   setv(\n\u001b[1;32m    630\u001b[0m       to_object,\n\u001b[1;32m    631\u001b[0m       [\u001b[39m'\u001b[39m\u001b[39mgenerationConfig\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m--> 632\u001b[0m       _GenerateContentConfig_to_mldev(\n\u001b[1;32m    633\u001b[0m           api_client, getv(from_object, [\u001b[39m'\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m'\u001b[39;49m]), to_object\n\u001b[1;32m    634\u001b[0m       ),\n\u001b[1;32m    635\u001b[0m   )\n\u001b[1;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m to_object\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/google/genai/models.py:509\u001b[0m, in \u001b[0;36m_GenerateContentConfig_to_mldev\u001b[0;34m(api_client, from_object, parent_object)\u001b[0m\n\u001b[1;32m    497\u001b[0m   setv(\n\u001b[1;32m    498\u001b[0m       to_object,\n\u001b[1;32m    499\u001b[0m       [\u001b[39m'\u001b[39m\u001b[39mresponseMimeType\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    500\u001b[0m       getv(from_object, [\u001b[39m'\u001b[39m\u001b[39mresponse_mime_type\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m    501\u001b[0m   )\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m getv(from_object, [\u001b[39m'\u001b[39m\u001b[39mresponse_schema\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m   setv(\n\u001b[1;32m    505\u001b[0m       to_object,\n\u001b[1;32m    506\u001b[0m       [\u001b[39m'\u001b[39m\u001b[39mresponseSchema\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    507\u001b[0m       _Schema_to_mldev(\n\u001b[1;32m    508\u001b[0m           api_client,\n\u001b[0;32m--> 509\u001b[0m           t\u001b[39m.\u001b[39;49mt_schema(api_client, getv(from_object, [\u001b[39m'\u001b[39;49m\u001b[39mresponse_schema\u001b[39;49m\u001b[39m'\u001b[39;49m])),\n\u001b[1;32m    510\u001b[0m           to_object,\n\u001b[1;32m    511\u001b[0m       ),\n\u001b[1;32m    512\u001b[0m   )\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m getv(from_object, [\u001b[39m'\u001b[39m\u001b[39mrouting_config\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mrouting_config parameter is not supported in Gemini API.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/google/genai/_transformers.py:828\u001b[0m, in \u001b[0;36mt_schema\u001b[0;34m(client, origin)\u001b[0m\n\u001b[1;32m    826\u001b[0m   process_schema(schema, client)\n\u001b[1;32m    827\u001b[0m   schema \u001b[39m=\u001b[39m schema[\u001b[39m'\u001b[39m\u001b[39mproperties\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mplaceholder\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 828\u001b[0m   \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mSchema\u001b[39m.\u001b[39;49mmodel_validate(schema)\n\u001b[1;32m    830\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnsupported schema type: \u001b[39m\u001b[39m{\u001b[39;00morigin\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-gemini/lib/python3.10/site-packages/pydantic/main.py:703\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m by_alias \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m by_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     \u001b[39mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    699\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    700\u001b[0m         code\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidate-by-alias-and-name-false\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    701\u001b[0m     )\n\u001b[0;32m--> 703\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(\n\u001b[1;32m    704\u001b[0m     obj, strict\u001b[39m=\u001b[39;49mstrict, from_attributes\u001b[39m=\u001b[39;49mfrom_attributes, context\u001b[39m=\u001b[39;49mcontext, by_alias\u001b[39m=\u001b[39;49mby_alias, by_name\u001b[39m=\u001b[39;49mby_name\n\u001b[1;32m    705\u001b[0m )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Schema\nitems.properties.table_contents.additionalProperties\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "class Table(BaseModel):\n",
    "    table_title: str = Field(description=\"title of the figure or table\")\n",
    "    table_contents: dict = Field(description=\"the data within the table as a dictionary\")\n",
    "\n",
    "prompt = f\"Extract the table 1 from this paper:\"\n",
    "# Example usage with the previous response\n",
    "table_contents = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-preview-04-17',\n",
    "        contents=[types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt],\n",
    "        config={\n",
    "            'response_mime_type': 'application/json',\n",
    "            'response_schema': list[Table],\n",
    "        },\n",
    "    )\n",
    "\n",
    "table_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAs Markdown:\")\n",
    "print(table.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples with PDFs [here](https://ai.google.dev/gemini-api/docs/document-processing?lang=python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Gemini Docs for All Capabilities\n",
    "\n",
    "- [Audio Understanding](https://ai.google.dev/gemini-api/docs/audio)\n",
    "- [Video Understanding](https://ai.google.dev/gemini-api/docs/video-understanding)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
