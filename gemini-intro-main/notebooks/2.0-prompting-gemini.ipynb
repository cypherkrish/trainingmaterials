{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Prompt Engineering with Gemini\n",
    "\n",
    "*   **Core Idea:** Prompt engineering is the iterative process of designing effective inputs (prompts) to guide Large Language Models (LLMs) toward desired outputs. It's essential because LLMs are prediction engines, and the prompt sets the context for that prediction.\n",
    "*   **Accessibility:** You don't need to be a data scientist; anyone can write prompts, but crafting *effective* ones takes practice and iteration.\n",
    "*   **Goal:** To create prompts that are clear, specific, and provide sufficient context, leading to accurate, relevant, and useful LLM responses. Inadequate prompts cause ambiguity and poor results.\n",
    "*   **Scope:** This guide focuses on prompting models like Gemini directly (via API or tools like Vertex AI Studio) where configuration is accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Essential LLM Output Configuration\n",
    "\n",
    "*Before* focusing solely on the prompt text, configure the model's output parameters. These significantly impact the results:\n",
    "\n",
    "*   **Output Length (Max Tokens):**\n",
    "    *   Sets the maximum number of tokens the model will generate.\n",
    "    *   **Practical Tip:** Be mindful of costs, latency, and energy use (more tokens = higher). Don't rely on this alone for succinctness; adjust the prompt too. Crucial for techniques like ReAct to prevent excessive output. Too short can truncate output (e.g., invalid JSON).\n",
    "*   **Sampling Controls (Temperature, Top-K, Top-P):** These control the randomness and creativity of the output.\n",
    "    *   **Temperature:**\n",
    "        *   Controls randomness. Lower values (~0.1-0.3) = more deterministic, focused, factual. Higher values (~0.7-1.0) = more creative, diverse, potentially unexpected.\n",
    "        *   **Practical Tip:** Use `0` for tasks with a single correct answer (math, strict data extraction). Start around `0.2` for factual but slightly flexible tasks, and `0.7-0.9` for creative tasks. Be wary of very high temps causing incoherence or the \"repetition loop bug\".\n",
    "    *   **Top-K:**\n",
    "        *   Considers only the `K` most likely next tokens. Lower `K` = more restricted/conservative. Higher `K` = more diverse. `K=1` is deterministic (like Temp 0).\n",
    "        *   **Practical Tip:** Start around `30-40`. Lower `K` (~20) for more factual, higher `K` (~40+) for creative.\n",
    "    *   **Top-P (Nucleus Sampling):**\n",
    "        *   Considers the smallest set of tokens whose cumulative probability exceeds `P`. Lower `P` = more conservative. Higher `P` (~0.95-1.0) = more diverse. `P=0` (or very small) often defaults to the single most likely token. `P=1` considers all tokens.\n",
    "        *   **Practical Tip:** Often used *instead* of or *with* Top-K. A common starting point is `0.95`. Lower `P` (~0.9) for factual, higher `P` (~0.99) for creative.\n",
    "    *   **Putting it Together:** The model typically filters by Top-K and Top-P first, then applies Temperature to the remaining candidates. Extreme settings in one can make others irrelevant (e.g., Temp 0 ignores K/P; K=1 ignores Temp/P).\n",
    "    *   **Starting Point Recommendation:** Temp `0.2`, Top-P `0.95`, Top-K `30` for balanced results. Adjust based on desired creativity/factuality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompting Techniques\n",
    "\n",
    "*   **Zero-Shot Prompting:**\n",
    "    *   Provide only the task description or question without any examples.\n",
    "    *   `Example: Classify the following movie review: [Review Text]`\n",
    "    *   **Practical Tip:** Simplest method, good starting point. May fail for complex tasks or when specific output formats are needed.\n",
    "*   **One-Shot / Few-Shot Prompting:**\n",
    "    *   Provide one (one-shot) or multiple (few-shot) examples of the task and desired output.\n",
    "    *   `Example (Few-Shot Sentiment):`\n",
    "        `Review: \"Loved it!\" Sentiment: Positive`\n",
    "        `Review: \"Boring.\" Sentiment: Negative`\n",
    "        `Review: \"It was okay.\" Sentiment: Neutral`\n",
    "        `Review: \"[New Review Text]\" Sentiment:`\n",
    "    *   **Practical Tip:** Highly effective for guiding the model on structure, style, and task logic. Use 3-5 high-quality, diverse examples as a rule of thumb. Include edge cases if needed. Ensure examples are accurate, as errors confuse the model.\n",
    "\n",
    "*   **System, Contextual, and Role Prompting:**\n",
    "    *   **System Prompt:** Defines the overall task, fundamental purpose, or constraints (e.g., \"Translate the following text to French.\", \"Only return JSON.\").\n",
    "    *   **Contextual Prompt:** Provides specific background information relevant to the *current* task or query (e.g., \"Given the previous conversation about user preferences, suggest a suitable product.\").\n",
    "    *   **Role Prompt:** Assigns a persona or identity to the LLM (e.g., \"Act as a pirate.\", \"You are a helpful travel guide specialized in budget travel.\").\n",
    "    *   **Practical Tip:** Use **Role Prompting** to control tone, style, and expertise (e.g., \"Explain this concept like I'm five.\", \"Write in a formal, academic style.\"). Combine these types as needed (e.g., a Role prompt can include Context).\n",
    "*   **Step-Back Prompting:**\n",
    "    *   Ask the LLM a more general, abstract question related to the specific task *first*. Then, use the answer to that general question as context when asking the specific task prompt.\n",
    "    *   **Practical Tip:** Improves reasoning by activating broader knowledge. Useful for complex problems or mitigating bias. Requires two LLM calls.\n",
    "*   **Chain of Thought (CoT) Prompting:**\n",
    "    *   Instruct the LLM to break down its reasoning process step-by-step before giving the final answer. Simply add phrases like \"Let's think step by step.\"\n",
    "    *   `Example: Q: [Math Problem]. Let's think step by step. A:`\n",
    "    *   **Practical Tip:** Significantly improves performance on tasks requiring reasoning (math, logic puzzles). Provides interpretability. Works well combined with few-shot examples showing the reasoning steps. Use **Temperature 0** for CoT tasks. Ensure the final answer comes *after* the reasoning steps. More tokens = higher cost/latency.\n",
    "*   **Self-Consistency:**\n",
    "    *   An enhancement to CoT. Run the same CoT prompt multiple times with a higher temperature (to generate diverse reasoning paths). Select the most frequent final answer (majority vote).\n",
    "    *   **Practical Tip:** Improves accuracy over basic CoT, especially for complex reasoning. Significantly increases cost due to multiple runs.\n",
    "*   **Tree of Thoughts (ToT):**\n",
    "    *   (Advanced) Explores multiple reasoning paths simultaneously, forming a tree structure. Better for complex exploration tasks. Less common in basic prompt engineering.\n",
    "*   **ReAct (Reason + Act):**\n",
    "    *   Enables LLMs to use external tools (like search APIs, code interpreters) by interleaving reasoning steps (`Thought:`) with actions (`Action:`, `Action Input:`) and observing results (`Observation:`).\n",
    "    *   **Practical Tip:** Foundational for building agents. Requires external frameworks (e.g., LangChain) and tool setup (API keys). Needs careful management of the prompt history (context) sent back to the LLM in each step. Restrict output length to avoid runaway actions.\n",
    "*   **Automatic Prompt Engineering (APE):**\n",
    "    *   Use an LLM to generate variations of an initial prompt for a specific task. Evaluate these generated prompts (manually or using metrics like BLEU/ROUGE) and select the best one.\n",
    "    *   **Practical Tip:** Can help discover effective prompt phrasing, especially for training data generation. Iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code Prompting Specifics\n",
    "\n",
    "*   LLMs like Gemini can understand and generate code.\n",
    "*   **Use Cases:**\n",
    "    *   **Writing Code:** Provide a description of the desired functionality. (e.g., \"Write a Python script to rename files in a folder, prepending 'draft_'\").\n",
    "    *   **Explaining Code:** Paste code and ask for an explanation. (e.g., \"Explain this Bash script line by line.\").\n",
    "    *   **Translating Code:** Provide code in one language and ask for another. (e.g., \"Translate this Bash script to Python.\").\n",
    "    *   **Debugging & Reviewing Code:** Provide code and the error message, ask for debugging help, or ask for general improvements/review.\n",
    "*   **Practical Tips:**\n",
    "    *   **ALWAYS TEST GENERATED CODE.** LLMs can make subtle or significant errors.\n",
    "    *   Be specific about the language, libraries, and desired functionality.\n",
    "    *   For debugging, provide the full error message and relevant code snippet.\n",
    "    *   In tools like Vertex AI Studio, use the 'Markdown' view for code output to preserve formatting (especially Python indentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Effective Prompting\n",
    "\n",
    "*   **Provide Examples (Few-Shot):** (Reiteration) Often the single most effective technique. Show, don't just tell.\n",
    "*   **Design with Simplicity:** Clear, concise language. Avoid jargon or unnecessary info. If it's confusing to you, it's likely confusing to the model.\n",
    "    *   **Tip:** Use clear action verbs (e.g., `Summarize`, `Classify`, `Generate`, `Translate`, `Extract`, `Rewrite`).\n",
    "*   **Be Specific About the Output:** Clearly define the desired format, length, style, content, and target audience. Don't be vague (e.g., \"Write a 3-paragraph blog post for beginners...\" vs. \"Write about consoles.\").\n",
    "*   **Use Instructions over Constraints:** Tell the model *what to do* rather than only *what not to do*. Constraints are okay for safety guardrails or strict formatting but can be less effective or conflicting.\n",
    "    *   `DO: Summarize the text in 3 bullet points.`\n",
    "    *   `LESS EFFECTIVE: Do not write a long summary. Do not use paragraphs.`\n",
    "*   **Control Max Token Length:** Use configuration or specify length in the prompt (e.g., \"...in under 100 words,\" \"...in a single sentence\").\n",
    "*   **Use Variables in Prompts:** Use placeholders (like `{city}` or `$user_input`) to make prompts reusable and dynamic. Essential for integrating prompts into applications.\n",
    "*   **Experiment Iteratively:** Try different phrasing, formats (question vs. instruction), styles, examples, configurations, and even different models/versions. Prompt engineering is not a one-shot process.\n",
    "*   **Mix Classes (Few-Shot Classification):** When providing examples for classification, ensure the examples cover different classes and aren't all clustered together to avoid order bias.\n",
    "*   **Adapt to Model Updates:** Newer model versions may have different capabilities or respond differently. Re-test prompts with new versions.\n",
    "*   **Experiment with Output Formats (JSON/XML):**\n",
    "    *   For non-creative tasks (extraction, classification, structured data), explicitly ask for output in JSON or XML.\n",
    "    *   **Benefits:** Consistent structure, easier parsing in applications, can enforce data types, reduces hallucination likelihood.\n",
    "    *   **Tip:** Provide the desired schema or an example JSON structure in the prompt (few-shot). Be mindful of token limits, as JSON is verbose. Use tools like the `json-repair` library (Python) to fix truncated/malformed JSON output.\n",
    "*   **Working with Schemas (Input):** Provide a JSON Schema definition along with the JSON input data. This helps the LLM understand the structure and focus on relevant fields, especially for complex or large inputs.\n",
    "*   **Collaborate:** If possible, have multiple people attempt prompt design and compare results.\n",
    "*   **DOCUMENT EVERYTHING:**\n",
    "    *   **Crucial:** Keep detailed records of your prompt attempts.\n",
    "    *   **Template Fields:** Prompt Name/Version, Goal, Model Used, Temperature, Top-K, Top-P, Max Tokens, Full Prompt Text, Output(s), Outcome (OK/Not OK/Sometimes OK), Feedback/Notes, Hyperlink (if saved in a tool like Vertex AI Studio).\n",
    "    *   **Why:** Enables learning, debugging, re-testing on new models, and avoids re-doing work.\n",
    "    *   **Tip:** Store prompts in separate files from application code for maintainability. Consider automated testing/evaluation for prompts in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Takeaway\n",
    "\n",
    "Effective prompt engineering is an iterative cycle: **Craft -> Test -> Analyze -> Document -> Refine.** It requires understanding the LLM's configuration options, leveraging different prompting techniques (especially examples), clearly stating intent, and meticulously documenting experiments to achieve consistent, high-quality results.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
